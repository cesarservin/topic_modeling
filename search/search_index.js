var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"topic_modeling","text":""},{"location":"index.html#usage","title":"Usage","text":"<ul> <li>Basic Usage: Run <code>topic_modeling.exe</code> or use <code>topic_modeling_main.ipynb</code></li> <li>Configuration: Modify pyproject.toml to add or remove packages.</li> </ul>"},{"location":"index.html#data","title":"Data","text":"<ul> <li>Sources: The dataset is collected by CPG distributor public site.</li> <li>Structure: Table of key features</li> </ul> <p>Example</p> <p>Input data format</p> Text <code>string</code>"},{"location":"index.html#result","title":"Result \u2705","text":"<ul> <li>Findings:</li> <li> <p>Based on the model the results of topics and words are different. This is not surprising as these methods are using different approaches under the hood.</p> <p>Overall, I would prefer to go with the LDA method as is has been a well stablished methodn on this field.</p> </li> <li> <p>Visualizations:</p> </li> <li>Example visualizations (if applicable).   </li> </ul>"},{"location":"index.html#directory-structure","title":"Directory Structure","text":"<pre><code>.\n\u251c\u2500\u2500 docs &lt;- markdown files for mkdocs\n\u2502   \u2514\u2500\u2500 img &lt;- assets\n\u251c\u2500\u2500 notebooks &lt;- jupyter notebooks for exploratory analysis and explanation\n\u2514\u2500\u2500 src - scripts for processing data eg. transformations, dataset merges etc.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data &lt;- loading, saving and modelling your data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 features &lt;- feature engineering\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model &lt;- algorithms and models\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 plots &lt;- plots\n\u2502   \u2514\u2500\u2500 utils &lt;- api and other\n\u251c\u2500\u2500 LICENSE &lt;- License\n\u251c\u2500\u2500 mkdocs.yml &lt;- config for mkdocs\n\u251c\u2500\u2500 pyproject.yml &lt;- config project\n\u2514\u2500\u2500 README.md &lt;- README file of the package\n</code></pre>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>To contribute create a PR a use conventional commits</p> <pre><code>fix: &lt;description&gt;\nfeat: &lt;description&gt;\ndocs: &lt;description&gt;\nrefactor: &lt;description&gt;\n</code></pre> <p>License</p> <p>The project is licensed under the MIT License.</p> <p>I hope this is helpful!</p>"},{"location":"data.html","title":"Data","text":""},{"location":"data.html#data.etl","title":"<code>data.etl</code>","text":"<p>General ETL process to move from interm to processed file add data to deployed stage</p>"},{"location":"data.html#data.etl.apply_function_to_non_integer_columns","title":"<code>apply_function_to_non_integer_columns(df, func)</code>","text":"<p>Applies the given function to each column in the DataFrame that is object type dtype. Used for cleaning up text data in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to process.</p> required <code>func</code> <code>callable</code> <p>The function to apply to each non-integer column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with non-integer columns processed by the given function.</p> Source code in <code>src/data/etl.py</code> <pre><code>def apply_function_to_non_integer_columns(df: pd.DataFrame, func) -&gt; pd.DataFrame:\n    \"\"\"\n    Applies the given function to each column in the DataFrame that is object type dtype.\n    Used for cleaning up text data in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to process.\n        func (callable): The function to apply to each non-integer column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with non-integer columns processed by the given function.\n    \"\"\"\n    for col in df.columns:\n        if df[col].dtype == \"object\":  # Check if column contains non-integer data\n            print(f\"Processing column: {col}\")\n            df[col] = df[col].apply(func)\n    return df\n</code></pre>"},{"location":"data.html#data.etl.backup_file","title":"<code>backup_file(path_csv_deployed, dst)</code>","text":"<p>copies file for archives</p> <p>Parameters:</p> Name Type Description Default <code>path_csv_deployed</code> <code>str</code> <p>path of file to back up</p> required <code>dst</code> <code>str</code> <p>path destination of file to save to</p> required Source code in <code>src/data/etl.py</code> <pre><code>def backup_file(path_csv_deployed: str, dst: str) -&gt; None:\n    \"\"\"copies file for archives\n\n    Args:\n        path_csv_deployed (str): path of file to back up\n        dst (str): path destination of file to save to\n    \"\"\"\n    import shutil\n\n    shutil.copy(path_csv_deployed, dst)\n</code></pre>"},{"location":"data.html#data.etl.csv_combine_proc","title":"<code>csv_combine_proc(paths)</code>","text":"<p>combines all datasets from the interim stage</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> <p>paths from interim datasets</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: combined dataframe</p> Source code in <code>src/data/etl.py</code> <pre><code>def csv_combine_proc(paths: list) -&gt; pd.DataFrame:\n    \"\"\"combines all datasets from the interim stage\n\n    Args:\n        paths (list): paths from interim datasets\n\n    Returns:\n        pd.DataFrame: combined dataframe\n    \"\"\"\n    import datetime\n\n    import pandas as pd\n\n    df = pd.DataFrame()\n    for file in paths:\n        filename = file.split(\"\\\\\")[8].split(\".\")[0]\n        print(\"Folder - \" + filename)\n\n        try:\n            df_temp = pd.read_csv(file)\n            df_temp[\"Source.Name.Interim\"] = filename\n\n            now = datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%d\")\n            # date ran\n            df_temp[\"proccessed\"] = now\n            df = pd.concat([df, df_temp], axis=0)\n\n        except pd.errors.EmptyDataError:\n            print(\"Folder \" + filename + \" is blank. Skipping file.\")\n    return df\n</code></pre>"},{"location":"data.html#data.etl.csv_combine_update_dep","title":"<code>csv_combine_update_dep(paths, path_csv_deployed, ref_col)</code>","text":"<p>combines datasets from deployed and processed stage removing     duplicated files from deployed stage if processed file     has same file name (considers for updated data in new files).     CONFIRM file names are the SAME if not it will     duplicate data.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> <p>paths from processed datasets</p> required <code>path_csv_deployed</code> <code>str</code> <p>path of deployed dataset</p> required <code>ref_col</code> <code>str</code> <p>reference column to avoid duplicated dated</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: combined dataset from processed and existing deployed</p> Source code in <code>src/data/etl.py</code> <pre><code>def csv_combine_update_dep(paths: list, path_csv_deployed: str, ref_col: str) -&gt; pd.DataFrame:\n    \"\"\"combines datasets from deployed and processed stage removing\n        duplicated files from deployed stage if processed file\n        has same file name (considers for updated data in new files).\n        CONFIRM file names are the SAME if not it will\n        duplicate data.\n\n    Args:\n        paths (list): paths from processed datasets\n        path_csv_deployed (str): path of deployed dataset\n        ref_col (str): reference column to avoid duplicated dated\n\n    Returns:\n        pd.DataFrame: combined dataset from processed and existing deployed\n    \"\"\"\n    import datetime\n\n    import pandas as pd\n\n    df_deployed = pd.read_csv(path_csv_deployed)\n\n    for file in paths:\n        filename = file.split(\"\\\\\")[8]\n        print(filename)\n\n        df_temp = pd.read_csv(file)\n\n        # date ran\n        now = datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%d\")\n        df_temp[\"deployed\"] = now\n\n        # v2\n        # removes files with the same file path in deployed\n        # if it reuploads it keeps one file (help with updates and duplicated files)\n        filenames = df_deployed[ref_col]\n\n        # unique set of deployed file names\n        filenames = set(filenames)\n\n        filenames_temp = df_temp[ref_col]\n\n        # unique set of processed file names\n        filenames_temp = set(filenames_temp)\n        # find matching names\n        updated = filenames.intersection(filenames_temp)\n        print(\"Updating ...\")\n        print(updated)\n        # remove matching file names based on the ref_col\n        df_deployed = df_deployed.loc[~df_deployed[ref_col].isin(updated)]\n\n        # combine datasets\n        df_deployed = pd.concat([df_deployed, df_temp], axis=0)\n\n    return df_deployed\n</code></pre>"},{"location":"data.html#data.etl.csv_dep_init","title":"<code>csv_dep_init(paths)</code>","text":"<p>Initilizes dataset to next stage to deployment from proccessed</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list</code> <p>paths from processed datasets</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: dataset from proccessed initialized</p> Source code in <code>src/data/etl.py</code> <pre><code>def csv_dep_init(paths: list) -&gt; pd.DataFrame:\n    \"\"\"Initilizes dataset to next stage to deployment from proccessed\n\n    Args:\n        paths (list): paths from processed datasets\n\n    Returns:\n        pd.DataFrame: dataset from proccessed initialized\n    \"\"\"\n    import datetime\n\n    import pandas as pd\n\n    for file in paths:\n        filename = file.split(\"\\\\\")[8]\n        print(filename)\n\n        df_temp = pd.read_csv(file)\n\n        # date ran\n        now = datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%d\")\n        df_temp[\"deployed\"] = now\n\n    return df_temp\n</code></pre>"},{"location":"data.html#data.etl.datafile_path_finder","title":"<code>datafile_path_finder(file_name)</code>","text":"<p>Constructs a path by combining the parent directory of the current working directory with the 'data' folder and the provided file name. If no file name is provided, a default path is returned.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file for which the path is to be determined.</p> required <p>Returns:</p> Name Type Description <code>df_dir</code> <code>str</code> <p>The full path to the file, or an indication if no file name was provided.</p> Source code in <code>src/data/etl.py</code> <pre><code>def datafile_path_finder(file_name: str) -&gt; str:\n    \"\"\"\n    Constructs a path by combining the parent directory of the current working directory with the 'data' folder\n    and the provided file name. If no file name is provided, a default path is returned.\n\n    Args:\n        file_name (str): The name of the file for which the path is to be determined.\n\n    Returns:\n        df_dir (str): The full path to the file, or an indication if no file name was provided.\n    \"\"\"\n    import glob\n    import os\n\n    main_dir = os.path.dirname(os.getcwd())\n    rawdata_dir = os.path.join(main_dir, \"data\", file_name)\n    df_dir = glob.glob(rawdata_dir)[0]\n    return df_dir\n</code></pre>"},{"location":"data.html#data.etl.find_nan","title":"<code>find_nan(df)</code>","text":"<p>finds all NaN values in a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe to search for NaN values</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: count of NaN values in each column</p> Source code in <code>src/data/etl.py</code> <pre><code>def find_nan(df : pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"finds all NaN values in a dataframe\n\n    Args:\n        df (pd.DataFrame): dataframe to search for NaN values\n\n    Returns:\n        pd.DataFrame: count of NaN values in each column\n    \"\"\"\n\n    return df.isnull().sum()\n</code></pre>"},{"location":"data.html#data.etl.remove_newline_tabs_spaces","title":"<code>remove_newline_tabs_spaces(text)</code>","text":"<p>Removes newlines and tabs from a string and replaces them with spaces</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text with newlines and tabs</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>cleaned text</p> Source code in <code>src/data/etl.py</code> <pre><code>def remove_newline_tabs_spaces(text : str) -&gt; str:\n    \"\"\"Removes newlines and tabs from a string and replaces them with spaces\n\n    Args:\n        text (str): text with newlines and tabs\n\n    Returns:\n        str: cleaned text\n    \"\"\"\n    # Replace newlines and tabs with spaces\n    text = re.sub(r\"[\\n\\t]+\", \" \", text)\n    # Optionally remove extra spaces\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n</code></pre>"},{"location":"model.html","title":"Models","text":""},{"location":"model.html#model.nlp","title":"<code>model.nlp</code>","text":""},{"location":"model.html#model.nlp.get_topic_words","title":"<code>get_topic_words(topic, topic_word_dist, vocab, topn=5)</code>","text":"<p>returns the top n words for a given topic from the topic model</p> <p>Parameters:</p> Name Type Description Default <code>topic</code> <code>int</code> <p>index of the topic</p> required <code>topic_word_dist</code> <code>ndarray</code> <p>word distribution for each topic</p> required <code>vocab</code> <code>ndarray</code> <p>vocabulary from vectorizer</p> required <code>topn</code> <code>int</code> <p>number of top words to return. Defaults to 10.</p> <code>5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: top n words for the given topic</p> Source code in <code>src/model/nlp.py</code> <pre><code>def get_topic_words(topic : int, topic_word_dist : np.ndarray, vocab : np.ndarray, topn : int =5) -&gt; np.ndarray:\n    \"\"\"returns the top n words for a given topic from the topic model\n\n    Args:\n        topic (int): index of the topic\n        topic_word_dist (np.ndarray): word distribution for each topic\n        vocab (np.ndarray): vocabulary from vectorizer\n        topn (int, optional): number of top words to return. Defaults to 10.\n\n    Returns:\n        np.ndarray: top n words for the given topic\n    \"\"\"\n\n    top_words = topic_word_dist[topic,:].argsort()[-topn:][::-1].tolist()\n    return vocab[top_words]\n</code></pre>"},{"location":"model.html#model.nlp.most_prevalent_topic","title":"<code>most_prevalent_topic(doc_topics)</code>","text":"<p>returns the most prevalent topic for each document</p> <p>Parameters:</p> Name Type Description Default <code>doc_topics</code> <code>ndarray</code> <p>fitted and transformed array of document topics</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: topic index for each document</p> Source code in <code>src/model/nlp.py</code> <pre><code>def most_prevalent_topic(doc_topics : np.ndarray) -&gt; np.ndarray:\n    \"\"\"returns the most prevalent topic for each document\n\n    Args:\n        doc_topics (np.ndarray): fitted and transformed array of document topics\n\n    Returns:\n        np.ndarray: topic index for each document\n    \"\"\"\n\n    return doc_topics.argmax(axis=1)\n</code></pre>"},{"location":"model.html#model.nlp.top_features_sm","title":"<code>top_features_sm(dtm, vec, n=10)</code>","text":"<p>Counts the top n features from a sparse matrix and returns a dictionary with the counts</p> <p>Parameters:</p> Name Type Description Default <code>dtm</code> <code>csr_matrix</code> <p>sparse matrix with the data</p> required <code>vec</code> <code>CountVectorizer</code> <p>sklearn countvetorizer to build vocabulary</p> required <code>n</code> <code>int</code> <p>number of top features to return. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: dictionary with the top features and their counts</p> Source code in <code>src/model/nlp.py</code> <pre><code>def top_features_sm(dtm : sp.sparse.csr_matrix, vec : sklearn.feature_extraction.text.CountVectorizer, n : int =10) -&gt; dict[str, int]:  # noqa: E501\n    \"\"\"Counts the top n features from a sparse matrix and returns a dictionary with the counts\n\n    Args:\n        dtm (sp.sparse.csr_matrix): sparse matrix with the data\n        vec (CountVectorizer): sklearn countvetorizer to build vocabulary\n        n (int, optional): number of top features to return. Defaults to 10.\n\n    Returns:\n        dict[str, int]: dictionary with the top features and their counts\n    \"\"\"\n\n    import numpy as np\n    words = np.array(vec.get_feature_names_out())\n    dtm = dtm.sum(axis=0)  # Sum across all documents to get the frequency of each feature\n    dtm = np.array(dtm).reshape(-1)\n    top_indices = dtm.argsort()[-n:]  # Get the indices of the top n features\n    top_indices = top_indices[::-1]  # Reverse the order of these indices\n\n    # Create a dictionary with feature names and their counts\n    top_features = {words[i]: dtm[i] for i in top_indices}\n    return top_features\n</code></pre>"},{"location":"model.html#model.nlp.topic_words_dist_ranked","title":"<code>topic_words_dist_ranked(topic_idx, topic_word_dist, vocab, num_words=10)</code>","text":"<p>returns the top n words for a given topic from the topic model</p> <p>Parameters:</p> Name Type Description Default <code>topic_idx</code> <code>int</code> <p>topic number</p> required <code>topic_word_dist</code> <code>ndarray</code> <p>distribution of words for each topic</p> required <code>vocab</code> <code>ndarray</code> <p>vocabulary from vectorizer</p> required <code>num_words</code> <code>int</code> <p>number of top words to return. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>word</p> Source code in <code>src/model/nlp.py</code> <pre><code>def topic_words_dist_ranked(topic_idx : int, topic_word_dist : np.ndarray, vocab : np.ndarray, num_words=10) -&gt; str:\n    \"\"\"returns the top n words for a given topic from the topic model\n\n    Args:\n        topic_idx (int): topic number\n        topic_word_dist (np.ndarray): distribution of words for each topic\n        vocab (np.ndarray): vocabulary from vectorizer\n        num_words (int, optional): number of top words to return. Defaults to 10.\n\n    Returns:\n        str: word\n    \"\"\"\n\n    top_words = topic_word_dist[topic_idx]\n    top_words_indices = top_words.argsort()[-num_words:][::-1]  # Get indices of top words\n    return [vocab[i] for i in top_words_indices]\n</code></pre>"}]}